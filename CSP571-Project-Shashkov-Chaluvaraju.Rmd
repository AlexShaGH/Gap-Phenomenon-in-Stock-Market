---
title: "CSP571 Summer 2022 Project. Gap Phenomenon in Stock Market: Evaluating the Significance of Price Gaps in Predicting Future Returns Using Statistical and Machine Learning Techniques"
output: 
  html_notebook: 
    toc: yes
    toc_float: yes
    toc_depth: 6
author: Oleksandr Shashkov and Santosh Chaluvaraju
        Illinois Institute of Technology
---

##LIBRARIES
```{r}
library(tibble)
library(collections)
library(tidyverse)
library(tidyquant)
library(lubridate)
library(dplyr)
library(timetk)
```

## SANTOSH CODE TO prepare dictionary (Please change the csv path) (Buggy Code needs fix)
```{r}
rm(list = ls())
```

```{r}
getList <- function(str, delimiter)
{
  return (unique(unlist(strsplit(str,delimiter))))
}
```

```{r}
df <- read.csv("/home/santosh/Desktop/MASTERS/STUDIES_MSDS/SEM_2_summer/Data_preparation_and_analysis/PROJECT/git_work/Gap-Phenomenon-in-Stock-Market/S&P 500 Historical Components & Changes(03-14-2022).csv")

#list of ticker list
listOfTkrList<- list()
for (x in 1:2648) {
  #getting ticker list of each row i.e. for a particular date 
    listOfTkrList[[x]] <- getList(df[x,2], ",")
}
#creating ticker period dictionary
tkrperiodDict <- dict()
x <- 1

#Iterating over list of ticker list
for (tkrList in listOfTkrList) {
  #Date of row x
  startDate <- df[x,1]
  
  #Iterating over each ticker in each list
  for (tkr in listOfTkrList[[x]]) {
    y <- x + 1
    #Iterating over each list after "tkrList" in list of lists i.e. listOfTkrList
    for (i in y:2648) {
      if(tkr %in% listOfTkrList[[i]]){
        index = which(listOfTkrList[[i]] == tkr)
        listOfTkrList[[i]] <- listOfTkrList[[i]][-index]
        endDate = df[i,1]
      }else{
        #If dictionary already has the key append the list
        if(tkrperiodDict$has(tkr)){
          periodList <- tkrperiodDict$get(tkr)
          tkrperiodDict$set(tkr, append(periodList, list(list(startDate, endDate))))
        }
        # ELSE make a new entry
        else{
          tkrperiodDict$set(tkr, list(list(startDate, endDate)))
        }
        break
      }
    }
    
  }
  
  x <- x + 1
  if(x == 2648){
    break
  }
  
}

```
```{r}
print(tkrperiodDict$get("AAL"))
```
## END SANtOSH CODE

# 1. Removing Survival Bias
```{r}
# mock up dictionary using list of lists to emulate ticker-ranges pairs
# tickers <- c("AAL","AAMRQ","AAPL")
# AAL1 <- list(as.Date("1/2/1996","%m/%d/%Y"),as.Date("1/13/1997","%m/%d/%Y"))
# AAL2 <- list(as.Date("3/23/2015","%m/%d/%Y"),as.Date("3/2/2022","%m/%d/%Y"))
# AAL <- list(AAL1,AAL2)
# AAMRQ <- list(list(as.Date("1/2/1996","%m/%d/%Y"),as.Date("3/10/2003","%m/%d/%Y")))
# AAPL <- list(list(as.Date("1/2/1996","%m/%d/%Y"),as.Date("3/2/2022","%m/%d/%Y")))
# ranges <- list(AAL,AAMRQ,AAPL)
# sp500_composition <- dict(items = ranges, keys = tickers)
# rm(AAL,AAL1,AAL2,AAMRQ,AAPL,ranges,tickers)
```

```{r}
# load the S&P500 historical composition
#SnP_components_raw <- read.csv("/home/santosh/Desktop/MASTERS/STUDIES_MSDS/SEM_2_summer/Data_preparation_and_analysis/PROJECT/git_work/Gap-Phenomenon-in-Stock-Market/S&P 500 Historical Components & Changes(03-14-2022).csv")

SnP_components_raw <- read.csv(file=file.choose())
```
```{r}
# form a set of all stocks tickers
tickers_set <- c()
for (composition in SnP_components_raw$tickers) {
  curr_composition <- strsplit(composition,split = ",")
  tickers_set <- unique(c(tickers_set,curr_composition[[1]]))
}
```
```{r}
# form a dictionary where a key is a ticker 
# and a value is a list of data ranges when a stock was in the S&P index

sp500_composition <- dict()
counter = 1
# iterate through the list of historical S&P components
for (tcr in tickers_set) {
  cat("Processing ticker:",tcr,"(",counter,")","\n")
  listed <- FALSE
  # declare time stamp variables
  start_timestamp <- Sys.Date()
  last_timestamp <- Sys.Date()
  # list of ranges when the current stock was a part of S&P index
  range_list <- list()
  # iterate through the time steps
  for (i in 1:nrow(SnP_components_raw)) {
    timestep <- SnP_components_raw[i,]
    curr_timestamp <- timestep$date[[1]][1]
    if (!(tcr %in% strsplit(timestep$tickers,split = ",")[[1]])){
      # ticker is not in the current composition
      if (listed){# stock was listed before but now it is not
        # form an entry in the list containing start and stop time stamps
        range_entry <- list(start_timestamp,last_timestamp)
        # add it to the list of ranges
        range_list <- append(range_list, list(range_entry))
        
        listed = FALSE
      }
    } else {# ticker is in the current composition
      if (!listed){# stock was not listed previously
        # start tracking new range
        start_timestamp <- curr_timestamp
        listed = TRUE
      }
    }
    last_timestamp <- curr_timestamp
  }
  # we are done with current ticker
  if (listed){  # need to check if the range needs to be closed
    # form an entry in the list containing start and stop time stamps
    range_entry <- list(start_timestamp,last_timestamp)
    # add it to the list of ranges
    range_list <- append(range_list, list(range_entry))
  }
  # add ticker and ranges to global dictionary
  sp500_composition$set(tcr,c(range_list))
  counter <- counter + 1
}

```


```{r}
# process dictionary to download stocks data for the ranges
for (stock in sp500_composition$keys()) {
  for (rng in sp500_composition$get(stock)){
    cat("ticker:",stock,"start:",
        format.Date(rng[[1]][1]),
        "stop:",format.Date(rng[[2]][1]),"\n")
    # download stocks data here having ticker and time stamps. tq_get returns data in tibble form, convert tibble data into dataframe before saving
    stock_prices_df <- as.data.frame(tq_get(stock, from = rng[[1]][1], to = rng[[2]][1]))
    # save it to the file
    file_name <- paste(stock,rng[[1]][1],"-",rng[[2]][1], sep = "-")
    #Putting all the files inside data folder
    file_name <- paste("data",file_name,sep = "/")
    file_name <- paste(file_name,"csv", sep = ".")
    write.csv(stock_prices_df , file_name, row.names = FALSE)
  }
}
```

#Downloading S&P500 index GSPC stock data
```{r}
gfpcdf <- as.data.frame(getSymbols("^GSPC", auto.assign = FALSE, from= "1985-01-01", src="yahoo"))
gfpcdf <- tibble::rownames_to_column(gfpcdf, "Date")
#Putting SPX data file inside data folder
file_name <- paste("data","SPX",sep = "/")
file_name <- paste(file_name,"csv", sep = ".")
write.csv(gfpcdf , file_name, row.names = FALSE)
```
# Alex's version of the same 
# But with the column names matching other stocks
```{r}
# get S&P500 data for the entire data range:
GSPC <- getSymbols("^GSPC", auto.assign=FALSE, from="1996-01-01", src="yahoo")
GSPC_data <- data.frame(symbol='GSPC',date=time(GSPC),as.matrix(GSPC))
colnames(GSPC_data ) <- c("symbol","date","open","high","low","close","volume","adjusted")
write.csv(GSPC_data,"C:\\Users\\Alex\\Documents\\IIT\\CSP571\\Assignments\\Project\\Gap-Phenomenon-in-Stock-Market\\data\\GSPC.csv", row.names = FALSE)
```

# NOT NECESSARY if your data folder has all the files downloaded from GIT, because git has data files which are appropriate
#Get all CSV files along with their full path
#Remove CSV files of tickers with in appropriate data
```{r}
path <- "/home/santosh/Desktop/MASTERS/STUDIES_MSDS/SEM_2_summer/Data_preparation_and_analysis/PROJECT/git_work/Gap-Phenomenon-in-Stock-Market/data" 
filesList <- list.files(path=path, pattern=NULL, all.files=FALSE, full.names=TRUE)

for (file in filesList) {
  #Remove files if it contains less than or equal to 2 columns
  if(ncol(read.csv(file)) <= 2){
    file.remove(file)
  }
  
}
```

#Get all CSV files along with their full path
```{r}
#path <- "/home/santosh/Desktop/MASTERS/STUDIES_MSDS/SEM_2_summer/Data_preparation_and_analysis/PROJECT/git_work/Gap-Phenomenon-in-Stock-Market/data"

path <- "C:\\Users\\Alex\\Documents\\IIT\\CSP571\\Assignments\\Project\\Gap-Phenomenon-in-Stock-Market\\data"
filesList <- list.files(path=path, pattern=NULL, all.files=FALSE, full.names=TRUE)
```

#Function to detect gaps
```{r}
detect_gaps <- function(last_h, last_l, h, l)
{
  gap_size = 0.0
  gap_type = "NONE"
  if(last_l > h){
    gap_type = "DOWN"
    gap_size = (h - last_l)/last_l
  }
  else if(last_h < l){
    gap_type = "UP"
    #gap_size = (l - last_h)/last_h
    gap_size = as.numeric((l - last_h)/last_h)    
  }
  gap_list = list(gap_type, gap_size)
  
  return (gap_list)
}
```

#Preprocessing SPX(S&P 500 index) ticker data
```{r}
spxdf <- read.csv("data/SPX.csv")
spxDailyReturns <- list()
for(i in 1:nrow(spxdf)){
  #calculating daily returns and storing it in spxDailyReturns list
  spxDailyReturns <- append(spxDailyReturns, (spxdf[i,5] - spxdf[i,2]) / spxdf[i,2])
  
}
#adding dailyReturns column
spxdf$spxdailyReturns <- spxDailyReturns

#Putting pre processed file inside pre_processed_data folder
file_name <- paste("SPX", "preprocessed", sep = "_" )
file_name <- paste("pre_processed_data",file_name,sep = "/")
file_name <- paste(file_name,"csv", sep = ".")
spxdf <- apply(spxdf,2,as.character)
write.csv(spxdf , file_name, row.names = FALSE)
```

```{r}
# loading data and calculating overnight returns for S&P500 index
spxdf <- read.csv("data/GSPC.csv")
rownames(spxdf) <- spxdf$date
for(i in 1:nrow(spxdf)){
      if(i == 1){
        # fake overnight return with intraday for first entry
        spxdf$return[i] <- (spxdf$close[i]-spxdf$open[i])/spxdf$open[i]
      }
      else{
        spxdf$return[i] <- (spxdf$adjusted[i]-spxdf$adjusted[i-1])/spxdf$adjusted[i-1]
      }
}
```


#Preprocessing: Detecting gaps and types, Calculating ranges, relative volume, daily returns and adjusted daily returns and adding it into new columns 
```{r}
for (file in filesList) {
  # load CSV file  
  df <- read.csv(file)
  
  # Perform preprocessing only if the data frame has more than 25 rows else ignore
  if(nrow(df) > 25){
    
    gap_list <- list()

    df <- df[!is.na(df$high),]
    df <- df[!is.na(df$low),]
    
    for(i in 1:nrow(df)){
      if(i == 1){
        gap_list[[i]] <- list("NONE", 0.0)
        df$vol_change[i] <- 0.0
        # fake overnight return with intraday for first entry
        df$return[i] <- (df$close[i]-df$open[i])/df$open[i]
      }
      else{
        gap_list[[i]] <- detect_gaps(df$high[i-1], df$low[i-1], df$high[i], df$low[i])
        # calculating relative volume
        df$vol_change[i] <- (df$volume[i] - df$volume[i-1])/df$volume[i-1]
        # calculating daily returns
        # We need overnight return, not intraday
        # And we need to use adjusted close for that
        df$return[i] <- (df$adjusted[i]-df$adjusted[i-1])/df$adjusted[i-1]
      }
      
      #calculating adjusted returns
      df$adjusted_return[i] <- df$return[i] - spxdf$return[which(rownames(spxdf)==df$date[i])]
      
    }
    df_temp <- as.data.frame(do.call(rbind, gap_list))
    colnames(df_temp) <- c("gap_type", "gap_size")
    df <- cbind(df, df_temp)
    #adding range column
    df$range <- (df$high - df$low)/df$low
    
    # extract day of the week and month as potential features
    df$weekday <- wday(df$date, week_start=1)
    df$month <- month(df$date)
    
    # calculate "candle body metric"
    df$candle_body_metric <- (df$close - df$open)/df$open

    # adding lags and leads
    selected_cols <- c(
      "vol_change","return","adjusted_return","range","weekday","month","candle_body_metric")
    lags_leads <- (-10:10)[as.logical( -10:10 != 0)]# -10...-1 1...10 - omit 0 
    df <- df %>% tk_augment_lags(contains(selected_cols), .lags = lags_leads)    

    # cleaning up rows with NAs
    df <- na.omit(df)

    #Putting all the pre processed files inside pre_processed_data folder
    file_name <- paste(sub('\\.csv$', '', basename(file)), "preprocessed", sep = "_" )
    file_name <- paste("pre_processed_data",file_name,sep = "/")
    file_name <- paste(file_name,"csv", sep = ".")
    df <- apply(df,2,as.character)
    write.csv(df , file_name, row.names = FALSE)
    # debug
    #break
  }
  
}
```
# TODO: save all the gaps into a single dataframe, preserve it inot CSV too

# TODO: run some statistical analysis, produce graphs - i.e. correlations, 
autocorrelation, distributions, etc. - anything to show some information about gaps

# TODO: Model it and try to predict !!!!
we may limit it to just one day after the gap for the purpose of the project